{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL with Python - Part(3-a).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN1k4/TrKN5Vla8WlkP95FV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veena-vijayan/DeepLearning-with-Python/blob/master/DL_with_Python_Part(3_a).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJzqdoWhQBsf",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning with Python**\n",
        "\n",
        "\n",
        "\n",
        "> Develop Deep Learning Models on Theano and Tensorflow Using Keras - Jason Brownlee\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngFmxStTQjHu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**PART III :: Multilayer Perceptrons** (part 1)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zGScyPe-cqFP"
      },
      "source": [
        "\n",
        "## **Chapter 6 - Crash Course In Multilayer Perceptrons**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se2yRoYIS3B3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   *Multilayer Perceptrons*<br>\n",
        "    *   Perceptron: a single neuron model that was a precursor to larger neural networks - it is a field of study that investigates how simple models of biological brains can be used to solve difficult computational tasks like\n",
        "the predictive modeling tasks we see in machine learning. \n",
        "    *   goal is not to create realistic models of the brain, but to develop robust algorithms and data structures to model difficult problems.\n",
        "    *   power of neural networks come from their ability to learn the representation in your training data and how to best relate it to the output variable that you want to predict i.e. it learn a mapping - they are capable of learning any mapping function and have been proven to be a universal approximation algorithm. \n",
        "    *   predictive capability of neural networks comes from the hierarchical or multilayered structure of the networks - data structure can pick out (learn to represent) features at different scales or resolutions and combine them into higher-order features.\n",
        "![picture](https://drive.google.com/uc?id=1vpGT5z-mS4L8H2n0gBqCeGgskgkcjv4a)\n",
        "\n",
        "*   *Neuron Weights*<br>\n",
        "    *   Like linear regression, each neuron also has a bias which can be thought of as an input that always has the value 1.0 and it too must be\n",
        "weighted. For example, a neuron may have two inputs in which case it requires three weights.\n",
        "    *   Weights are often initialized to small random values, such as values in the range 0 to 0.3 (more complex initialization schemes can also be used). \n",
        "    *   Like linear regression, larger weights indicate increased complexity and fragility of the model - desirable to keep weights in the network small and regularization techniques can be used.\n",
        "\n",
        "*   *Activation*<br>\n",
        "    *   weighted inputs are summed and passed through an activation function (transfer function) - a simple mapping of summed weighted input to the\n",
        "output of the neuron. \n",
        "    *   called an activation function because it governs the threshold at which the neuron is activated and the strength of the output signal (if summed input >0.5, then output a value 1 else 0)\n",
        "    *   Traditionally nonlinear activation functions are used - allows the network to combine the inputs in more complex ways and in turn provide a richer capability in the functions they can model.\n",
        "    *   example: \n",
        "        *   sigmoid function - output a value between 0 and 1 with an s-shaped distribution\n",
        "        *   Tanh - outputs the same distribution over the range -1 to +1.\n",
        "        *   the rectifier activation function (ReLu) returen input if input > 0 else 0.\n",
        "\n",
        "*   *Networks of Neurons*<br>\n",
        "    *   Neurons are arranged into networks of neurons - row of neurons is called a layer and one network can have multiple layers. \n",
        "    *   architecture of the neurons in the network is called the network  topology.\n",
        "![picture](https://drive.google.com/uc?id=1aDTl9ClcjZKtppNwWj0FGWJMBcOd14DV)\n",
        "    *   Layers of the network:\n",
        "      1.   Input or Visible Layers<br>\n",
        "           *   bottom layer that takes input from your dataset - the exposed part of the network. \n",
        "           *   usually drawn with one neuron per input value or column in your dataset - they simply pass the input value though to the next layer.\n",
        "      2.   Hidden Layers\n",
        "           *   layers after the input layer - they are not directly exposed to the input. \n",
        "           *   Deep learning can refer to having many hidden layers in\n",
        "your neural network. \n",
        "      3.   Output Layer\n",
        "           *   final hidden layer - responsible for outputting a value or  vector of values that correspond to the format required for the problem. \n",
        "           *   The choice of activation function in the output layer is constrained by the type of modelling problem. \n",
        "           *   For example:\n",
        "              *   regression problem - single output neuron and no activation function.\n",
        "              *   binary classiffication problem - single output neuron and sigmoid activation function.\n",
        "              *   multiclass classiffication problem - multiple neurons in the output layer (one for each class) and softmax activation function may be used to output a probability of the network predicting each of the class values. \n",
        "\n",
        "*   *Training Networks*<br>\n",
        "    * Data Preparation\n",
        "      *   Data must be numerical, for example real values. \n",
        "      *   categorical data can be converted to a real-valued representation called a one hot encoding.\n",
        "      *   one hot encoding can be used on the output variable in classification problems with more than one class - would create a binary vector from a single column that would be easy to directly compare to the output of the neuron in the network's output layer, which would output one value for each class. \n",
        "      *   Neural networks require the input to be scaled in a consistent way - either rescale it to the range between 0 and 1 called normalization or standardize it so that the distribution of each column has the\n",
        "mean of zero and the standard deviation of 1. \n",
        "      *   Scaling also applies to image pixel data. \n",
        "    * Stochastic Gradient Descent\n",
        "      *   preferred training algorithm for neural networks is called stochastic\n",
        "gradient descent - one row of data is exposed to the network at a time as input.\n",
        "      *   forward pass on the network - the network processes the input upward activating neurons as it goes to produce an output value - also the type of pass that is used after training to make predictions on new data.\n",
        "      *   Backpropagation algorithm - output of the network is compared to the expected output and an error is calculated - error is then propagated back through the network, one layer at a time, and the weights\n",
        "are updated according to the amount that they contributed to the error. \n",
        "      *   the process is repeated for all of the examples in the\n",
        "training data. One round of updating the network for the entire training dataset is called an epoch. A network may be trained for tens, hundreds or many thousands of epochs.\n",
        "    * Weight Updates\n",
        "      *   online learning - weights are updated from the errors calculated for each training example - can result in fast but also chaotic changes to the network.\n",
        "      *   batch learning - errors can be saved up across all of the training examples and the network can be updated at the end - often more stable.\n",
        "      *   size of the batch, the number of examples the network is shown before an update is often reduced to a small number, such as tens or hundreds of examples. \n",
        "      *   learning rate - the amount by which weights are updated - also called the step size and controls the step or change made to network weights for a given error - small learning rates are usually used such as 0.1 or 0.01 or smaller. \n",
        "      *   The update equation can be complemented with additional configuration terms that you can set:\n",
        "          *   Momentum - incorporates the properties from the previous weight update to allow the weights to continue to change in the same direction even when there is less error being calculated.\n",
        "          *   Learning Rate Decay - decrease the learning rate over epochs to allow the network to make large changes to the weights at the beginning and smaller fine tuning changes later in the training schedule.\n",
        "\n",
        "*   *Prediction*<br>\n",
        "    *   trained neural networks can be used to make predictions - you can make\n",
        "predictions on test or validation data in order to estimate the skill of the model on unseen data or deploy it operationally and use it to make predictions continuously.\n",
        "    *   network topology and the final set of weights is needed to be saved from the model. \n",
        "    *   predictions are made by providing the input to the network and performing a forward-pass allowing it to generate an output that you can use as a prediction.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEeODZoJVQUD",
        "colab_type": "text"
      },
      "source": [
        "> Summary\n",
        "\n",
        "\n",
        "\n",
        "In this lesson you discovered artificial neural networks for machine learning. You learned:\n",
        "1. How neural networks are not models of the brain but are instead computational models for solving complex machine learning problems.\n",
        "2. That neural networks are comprised of neurons that have weights and activation functions.\n",
        "3. The networks are organized into layers of neurons and are trained using stochastic gradient descent.\n",
        "4. That it is a good idea to prepare your data before training a neural network model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrTLvq6yVq_L"
      },
      "source": [
        "\n",
        "## **Chapter 7 - Develop Your First Neural Network With Keras**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3izSxepvVq_R"
      },
      "source": [
        "\n",
        "\n",
        "*   how to create your own models in the future - the steps you are going to cover in this tutorial are as follows:\n",
        "    1. Load Data:\n",
        "      *   dataset - Pima Indians onset of diabetes\n",
        "      *   binary classiffication problem (onset of diabetes as 1 or not as 0)\n",
        "      *   input variables are numerical and have varying scales - a total of 8  attributes are present.\n",
        "      *   a good idea to initialize the random number generator with a fixed seed value - so that you can run the same to get the same result - useful if you need to demonstrate a result, compare algorithms using the same source of randomness or to debug a part of your code.\n",
        "      *   load the file directly using the NumPy function loadtxt() \n",
        "      *   split the dataset into input variables (X) and the output class variable (Y)  \n",
        "    2. Define Model:\n",
        "      *  models in Keras are defined as a sequence of layers.\n",
        "      *  create a Sequential model and add layers one at a time.\n",
        "      *  ensure the input layer has the right number of inputs - specified when creating the first layer with the input dim argument and setting it to 8 for the 8 input variables\n",
        "      *  number of layers and their types - often the best network structure is found through a process of trial and error experimentation.\n",
        "      *  here we use a fully-connected network structure with three layers. Fully connected layers are defined using the Dense class. We can specify the number of neurons in the layer as the first argument and specify the activation function using the activation argument. We will use the rectifier (relu) activation function on the first two layers and the sigmoid activation function in the output layer. <br>\n",
        "![picture](https://drive.google.com/uc?id=1cVRkai29Nb4yGJ7I7AWH5qApwOwqB2ff)\n",
        "    3. Compile Model:\n",
        "      *  once the model is defined, we can compile it\n",
        "      *  compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow.\n",
        "      *  backend automatically chooses the best way to represent the network for training and making predictions - training a network means finding the\n",
        "best set of weights to make predictions for this problem.\n",
        "      *  when compiling, we must specify the *loss function - here, binary crossentropy* (use to evaluate a set of weights), the *optimizer - here adam* (used to search through different weights for the network) and optional *metrics - here, classiffication accuracy* (to collect and report during training). \n",
        "    4. Fit Model:\n",
        "      *  execute the model on some data\n",
        "      *  train or fit our model on our loaded data by calling the fit() function on the model.\n",
        "      *  must also specify *epochs - here 150* and *batch size - here, 10* using the epochs and batch size argument.\n",
        "    5. Evaluate Model:\n",
        "      *  evaluate the performance of the network on the same dataset -this will only give us an idea of how well we have modeled the dataset (e.g. train accuracy), but no idea of how well the algorithm might perform on new\n",
        "data - for simplicity, ideally to be split into train and test for the training and evaluation of your model.\n",
        "      *  evaluate your model on your training dataset using the evaluation() function and pass it the same input and output used to train the model - will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics, such as accuracy.\n",
        "    6. Tie It All Together:\n",
        "      *  the above steps are combined together into a complete code example.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRxiWg15UN8p",
        "colab_type": "code",
        "outputId": "2b69140b-8eab-449f-cf05-4d53cc3ccbc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Create your first MLP in Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "768/768 [==============================] - 0s 583us/step - loss: 3.1750 - accuracy: 0.5820\n",
            "Epoch 2/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.9557 - accuracy: 0.5729\n",
            "Epoch 3/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.7631 - accuracy: 0.6302\n",
            "Epoch 4/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.7226 - accuracy: 0.6471\n",
            "Epoch 5/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.6941 - accuracy: 0.6732\n",
            "Epoch 6/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.6683 - accuracy: 0.6836\n",
            "Epoch 7/150\n",
            "768/768 [==============================] - 0s 136us/step - loss: 0.6628 - accuracy: 0.6719\n",
            "Epoch 8/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.6483 - accuracy: 0.6836\n",
            "Epoch 9/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.6346 - accuracy: 0.7018\n",
            "Epoch 10/150\n",
            "768/768 [==============================] - 0s 118us/step - loss: 0.6429 - accuracy: 0.6797\n",
            "Epoch 11/150\n",
            "768/768 [==============================] - 0s 117us/step - loss: 0.6565 - accuracy: 0.6732\n",
            "Epoch 12/150\n",
            "768/768 [==============================] - 0s 116us/step - loss: 0.6524 - accuracy: 0.6641\n",
            "Epoch 13/150\n",
            "768/768 [==============================] - 0s 118us/step - loss: 0.6322 - accuracy: 0.6836\n",
            "Epoch 14/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.6219 - accuracy: 0.7005\n",
            "Epoch 15/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.6021 - accuracy: 0.7083\n",
            "Epoch 16/150\n",
            "768/768 [==============================] - 0s 116us/step - loss: 0.5873 - accuracy: 0.7018\n",
            "Epoch 17/150\n",
            "768/768 [==============================] - 0s 157us/step - loss: 0.5818 - accuracy: 0.6992\n",
            "Epoch 18/150\n",
            "768/768 [==============================] - 0s 140us/step - loss: 0.5941 - accuracy: 0.6979\n",
            "Epoch 19/150\n",
            "768/768 [==============================] - 0s 130us/step - loss: 0.5751 - accuracy: 0.7031\n",
            "Epoch 20/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.5744 - accuracy: 0.7383\n",
            "Epoch 21/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.5654 - accuracy: 0.7188\n",
            "Epoch 22/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5790 - accuracy: 0.7018\n",
            "Epoch 23/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5710 - accuracy: 0.7135\n",
            "Epoch 24/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.5676 - accuracy: 0.7253\n",
            "Epoch 25/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.5557 - accuracy: 0.7448\n",
            "Epoch 26/150\n",
            "768/768 [==============================] - 0s 118us/step - loss: 0.5731 - accuracy: 0.7148\n",
            "Epoch 27/150\n",
            "768/768 [==============================] - 0s 130us/step - loss: 0.5568 - accuracy: 0.7214\n",
            "Epoch 28/150\n",
            "768/768 [==============================] - 0s 135us/step - loss: 0.5580 - accuracy: 0.7214\n",
            "Epoch 29/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5779 - accuracy: 0.7188\n",
            "Epoch 30/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5625 - accuracy: 0.7214\n",
            "Epoch 31/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.5719 - accuracy: 0.7135\n",
            "Epoch 32/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.5673 - accuracy: 0.7161\n",
            "Epoch 33/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.5552 - accuracy: 0.7135\n",
            "Epoch 34/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.5544 - accuracy: 0.7253\n",
            "Epoch 35/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5558 - accuracy: 0.7201\n",
            "Epoch 36/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.5640 - accuracy: 0.7148\n",
            "Epoch 37/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.5351 - accuracy: 0.7331\n",
            "Epoch 38/150\n",
            "768/768 [==============================] - 0s 138us/step - loss: 0.5450 - accuracy: 0.7240\n",
            "Epoch 39/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5524 - accuracy: 0.7188\n",
            "Epoch 40/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.5500 - accuracy: 0.7214\n",
            "Epoch 41/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.5485 - accuracy: 0.7279\n",
            "Epoch 42/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5413 - accuracy: 0.7292\n",
            "Epoch 43/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.5364 - accuracy: 0.7357\n",
            "Epoch 44/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.5400 - accuracy: 0.7422\n",
            "Epoch 45/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.5366 - accuracy: 0.7591\n",
            "Epoch 46/150\n",
            "768/768 [==============================] - 0s 135us/step - loss: 0.5292 - accuracy: 0.7435\n",
            "Epoch 47/150\n",
            "768/768 [==============================] - 0s 134us/step - loss: 0.5375 - accuracy: 0.7344\n",
            "Epoch 48/150\n",
            "768/768 [==============================] - 0s 134us/step - loss: 0.5387 - accuracy: 0.7318\n",
            "Epoch 49/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.5384 - accuracy: 0.7474\n",
            "Epoch 50/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.5304 - accuracy: 0.7370\n",
            "Epoch 51/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.5329 - accuracy: 0.7448\n",
            "Epoch 52/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.5404 - accuracy: 0.7305\n",
            "Epoch 53/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.5398 - accuracy: 0.7396\n",
            "Epoch 54/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.5440 - accuracy: 0.7344\n",
            "Epoch 55/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5227 - accuracy: 0.7474\n",
            "Epoch 56/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.5326 - accuracy: 0.7409\n",
            "Epoch 57/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.5370 - accuracy: 0.7409\n",
            "Epoch 58/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.5279 - accuracy: 0.7500\n",
            "Epoch 59/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.5161 - accuracy: 0.7539\n",
            "Epoch 60/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5387 - accuracy: 0.7383\n",
            "Epoch 61/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5302 - accuracy: 0.7292\n",
            "Epoch 62/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.5219 - accuracy: 0.7461\n",
            "Epoch 63/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.5482 - accuracy: 0.7227\n",
            "Epoch 64/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.5403 - accuracy: 0.7383\n",
            "Epoch 65/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.5252 - accuracy: 0.7487\n",
            "Epoch 66/150\n",
            "768/768 [==============================] - 0s 119us/step - loss: 0.5122 - accuracy: 0.7396\n",
            "Epoch 67/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.5196 - accuracy: 0.7318\n",
            "Epoch 68/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.5206 - accuracy: 0.7500\n",
            "Epoch 69/150\n",
            "768/768 [==============================] - 0s 137us/step - loss: 0.5228 - accuracy: 0.7409\n",
            "Epoch 70/150\n",
            "768/768 [==============================] - 0s 142us/step - loss: 0.5377 - accuracy: 0.7227\n",
            "Epoch 71/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.5248 - accuracy: 0.7474\n",
            "Epoch 72/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5202 - accuracy: 0.7500\n",
            "Epoch 73/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5214 - accuracy: 0.7370\n",
            "Epoch 74/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.5184 - accuracy: 0.7578\n",
            "Epoch 75/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5169 - accuracy: 0.7513\n",
            "Epoch 76/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.5169 - accuracy: 0.7500\n",
            "Epoch 77/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5249 - accuracy: 0.7526\n",
            "Epoch 78/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.5185 - accuracy: 0.7643\n",
            "Epoch 79/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.5168 - accuracy: 0.7487\n",
            "Epoch 80/150\n",
            "768/768 [==============================] - 0s 154us/step - loss: 0.5158 - accuracy: 0.7565\n",
            "Epoch 81/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.5111 - accuracy: 0.7604\n",
            "Epoch 82/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.5021 - accuracy: 0.7539\n",
            "Epoch 83/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.5044 - accuracy: 0.7552\n",
            "Epoch 84/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.4977 - accuracy: 0.7630\n",
            "Epoch 85/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5060 - accuracy: 0.7448\n",
            "Epoch 86/150\n",
            "768/768 [==============================] - 0s 136us/step - loss: 0.5101 - accuracy: 0.7435\n",
            "Epoch 87/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.4991 - accuracy: 0.7539\n",
            "Epoch 88/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.5003 - accuracy: 0.7565\n",
            "Epoch 89/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.5043 - accuracy: 0.7747\n",
            "Epoch 90/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.5079 - accuracy: 0.7513\n",
            "Epoch 91/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.4987 - accuracy: 0.7513\n",
            "Epoch 92/150\n",
            "768/768 [==============================] - 0s 135us/step - loss: 0.5059 - accuracy: 0.7435\n",
            "Epoch 93/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.4966 - accuracy: 0.7591\n",
            "Epoch 94/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.4960 - accuracy: 0.7604\n",
            "Epoch 95/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.5078 - accuracy: 0.7357\n",
            "Epoch 96/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.4930 - accuracy: 0.7630\n",
            "Epoch 97/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.4923 - accuracy: 0.7760\n",
            "Epoch 98/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.4898 - accuracy: 0.7643\n",
            "Epoch 99/150\n",
            "768/768 [==============================] - 0s 158us/step - loss: 0.4908 - accuracy: 0.7695\n",
            "Epoch 100/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4875 - accuracy: 0.7682\n",
            "Epoch 101/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.4852 - accuracy: 0.7682\n",
            "Epoch 102/150\n",
            "768/768 [==============================] - 0s 135us/step - loss: 0.5034 - accuracy: 0.7500\n",
            "Epoch 103/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4951 - accuracy: 0.7565\n",
            "Epoch 104/150\n",
            "768/768 [==============================] - 0s 151us/step - loss: 0.4922 - accuracy: 0.7773\n",
            "Epoch 105/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.5238 - accuracy: 0.7461\n",
            "Epoch 106/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4979 - accuracy: 0.7578\n",
            "Epoch 107/150\n",
            "768/768 [==============================] - 0s 123us/step - loss: 0.4958 - accuracy: 0.7721\n",
            "Epoch 108/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.5023 - accuracy: 0.7617\n",
            "Epoch 109/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.4854 - accuracy: 0.7630\n",
            "Epoch 110/150\n",
            "768/768 [==============================] - 0s 127us/step - loss: 0.4879 - accuracy: 0.7565\n",
            "Epoch 111/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.4817 - accuracy: 0.7695\n",
            "Epoch 112/150\n",
            "768/768 [==============================] - 0s 140us/step - loss: 0.4903 - accuracy: 0.7656\n",
            "Epoch 113/150\n",
            "768/768 [==============================] - 0s 136us/step - loss: 0.5011 - accuracy: 0.7513\n",
            "Epoch 114/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.4911 - accuracy: 0.7435\n",
            "Epoch 115/150\n",
            "768/768 [==============================] - 0s 120us/step - loss: 0.4908 - accuracy: 0.7656\n",
            "Epoch 116/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.4911 - accuracy: 0.7695\n",
            "Epoch 117/150\n",
            "768/768 [==============================] - 0s 129us/step - loss: 0.4906 - accuracy: 0.7669\n",
            "Epoch 118/150\n",
            "768/768 [==============================] - 0s 136us/step - loss: 0.4928 - accuracy: 0.7695\n",
            "Epoch 119/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.4880 - accuracy: 0.7708\n",
            "Epoch 120/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.4975 - accuracy: 0.7565\n",
            "Epoch 121/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.4979 - accuracy: 0.7708\n",
            "Epoch 122/150\n",
            "768/768 [==============================] - 0s 139us/step - loss: 0.4807 - accuracy: 0.7591\n",
            "Epoch 123/150\n",
            "768/768 [==============================] - 0s 139us/step - loss: 0.4882 - accuracy: 0.7617\n",
            "Epoch 124/150\n",
            "768/768 [==============================] - 0s 142us/step - loss: 0.4838 - accuracy: 0.7695\n",
            "Epoch 125/150\n",
            "768/768 [==============================] - 0s 134us/step - loss: 0.4815 - accuracy: 0.7812\n",
            "Epoch 126/150\n",
            "768/768 [==============================] - 0s 122us/step - loss: 0.4796 - accuracy: 0.7565\n",
            "Epoch 127/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4911 - accuracy: 0.7539\n",
            "Epoch 128/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4705 - accuracy: 0.7630\n",
            "Epoch 129/150\n",
            "768/768 [==============================] - 0s 134us/step - loss: 0.4853 - accuracy: 0.7682\n",
            "Epoch 130/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.4679 - accuracy: 0.7839\n",
            "Epoch 131/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.4807 - accuracy: 0.7682\n",
            "Epoch 132/150\n",
            "768/768 [==============================] - 0s 129us/step - loss: 0.4793 - accuracy: 0.7773\n",
            "Epoch 133/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.4801 - accuracy: 0.7643\n",
            "Epoch 134/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.4819 - accuracy: 0.7643\n",
            "Epoch 135/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.4701 - accuracy: 0.7695\n",
            "Epoch 136/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4691 - accuracy: 0.7643\n",
            "Epoch 137/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.4661 - accuracy: 0.7734\n",
            "Epoch 138/150\n",
            "768/768 [==============================] - 0s 132us/step - loss: 0.4773 - accuracy: 0.7747\n",
            "Epoch 139/150\n",
            "768/768 [==============================] - 0s 141us/step - loss: 0.4710 - accuracy: 0.7773\n",
            "Epoch 140/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.4788 - accuracy: 0.7734\n",
            "Epoch 141/150\n",
            "768/768 [==============================] - 0s 124us/step - loss: 0.4681 - accuracy: 0.7695\n",
            "Epoch 142/150\n",
            "768/768 [==============================] - 0s 126us/step - loss: 0.4803 - accuracy: 0.7604\n",
            "Epoch 143/150\n",
            "768/768 [==============================] - 0s 125us/step - loss: 0.4730 - accuracy: 0.7721\n",
            "Epoch 144/150\n",
            "768/768 [==============================] - 0s 128us/step - loss: 0.4740 - accuracy: 0.7760\n",
            "Epoch 145/150\n",
            "768/768 [==============================] - 0s 134us/step - loss: 0.4883 - accuracy: 0.7721\n",
            "Epoch 146/150\n",
            "768/768 [==============================] - 0s 121us/step - loss: 0.4967 - accuracy: 0.7552\n",
            "Epoch 147/150\n",
            "768/768 [==============================] - 0s 140us/step - loss: 0.4831 - accuracy: 0.7773\n",
            "Epoch 148/150\n",
            "768/768 [==============================] - 0s 130us/step - loss: 0.4747 - accuracy: 0.7708\n",
            "Epoch 149/150\n",
            "768/768 [==============================] - 0s 131us/step - loss: 0.4730 - accuracy: 0.7630\n",
            "Epoch 150/150\n",
            "768/768 [==============================] - 0s 133us/step - loss: 0.4754 - accuracy: 0.7773\n",
            "768/768 [==============================] - 0s 48us/step\n",
            "\n",
            "accuracy: 78.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-TEjPyv6XgS7"
      },
      "source": [
        "\n",
        "> Summary\n",
        "\n",
        "In this lesson you discovered how to create your first neural network model using the powerful Keras Python library for deep learning. Specifically you learned the five key steps in using Keras to create a neural network or deep learning model, step-by-step including:\n",
        "1. How to load data.\n",
        "2. How to define a neural network model in Keras.\n",
        "3. How to compile a Keras model using the efficient numerical backend.\n",
        "4. How to train a model on data.\n",
        "5.  to evaluate a model on data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeNfHeXwc5Rv"
      },
      "source": [
        "\n",
        "## **Chapter 8 - Evaluate The Performance of Deep Learning Models**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgWax5-Wc5R4"
      },
      "source": [
        "*   need to make a number of decisions when designing and configuring your deep learning models - high-level decisions like the number, size and type of layers in your network and lower level decisions like the choice of loss function, activation functions, optimization procedure and number of epochs.\n",
        "*   need to have a robust test harness that allows you to estimate the performance of a given configuration on unseen data, and reliably compare the performance to other configurations.\n",
        "*   *Data Splitting*: typical to use a simple separation of data into training and test datasets or training and validation datasets. \n",
        "*   Keras provides two convenient ways of evaluating your deep learning algorithms this way:\n",
        "    1. Use an automatic verification dataset.\n",
        "        *   keras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset each epoch - can do this by setting the validation split argument on the fit() function to a percentage of the size of your training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o6S1W2e3Rb-",
        "colab_type": "code",
        "outputId": "5c46e3a2-ffc4-427e-b24a-5be7a739d2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MLP with automatic validation set\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 514 samples, validate on 254 samples\n",
            "Epoch 1/150\n",
            "514/514 [==============================] - 0s 475us/step - loss: 10.9429 - accuracy: 0.6381 - val_loss: 3.4832 - val_accuracy: 0.6102\n",
            "Epoch 2/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 1.4903 - accuracy: 0.4533 - val_loss: 0.9238 - val_accuracy: 0.4724\n",
            "Epoch 3/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.9798 - accuracy: 0.4455 - val_loss: 0.8273 - val_accuracy: 0.4921\n",
            "Epoch 4/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.8920 - accuracy: 0.4630 - val_loss: 0.8027 - val_accuracy: 0.4803\n",
            "Epoch 5/150\n",
            "514/514 [==============================] - 0s 168us/step - loss: 0.8339 - accuracy: 0.4844 - val_loss: 0.7750 - val_accuracy: 0.6772\n",
            "Epoch 6/150\n",
            "514/514 [==============================] - 0s 170us/step - loss: 0.7981 - accuracy: 0.6342 - val_loss: 0.7687 - val_accuracy: 0.6850\n",
            "Epoch 7/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.7545 - accuracy: 0.6459 - val_loss: 0.7606 - val_accuracy: 0.6654\n",
            "Epoch 8/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.7297 - accuracy: 0.6654 - val_loss: 0.7504 - val_accuracy: 0.6772\n",
            "Epoch 9/150\n",
            "514/514 [==============================] - 0s 167us/step - loss: 0.7164 - accuracy: 0.6654 - val_loss: 0.7422 - val_accuracy: 0.6732\n",
            "Epoch 10/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.6977 - accuracy: 0.6595 - val_loss: 0.7380 - val_accuracy: 0.6535\n",
            "Epoch 11/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.6863 - accuracy: 0.6556 - val_loss: 0.7387 - val_accuracy: 0.6457\n",
            "Epoch 12/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6736 - accuracy: 0.6615 - val_loss: 0.7428 - val_accuracy: 0.6457\n",
            "Epoch 13/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.6742 - accuracy: 0.6595 - val_loss: 0.7197 - val_accuracy: 0.6457\n",
            "Epoch 14/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.6590 - accuracy: 0.6654 - val_loss: 0.7208 - val_accuracy: 0.6378\n",
            "Epoch 15/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6535 - accuracy: 0.6751 - val_loss: 0.6926 - val_accuracy: 0.6575\n",
            "Epoch 16/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.6580 - accuracy: 0.6654 - val_loss: 0.7129 - val_accuracy: 0.6457\n",
            "Epoch 17/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6494 - accuracy: 0.6673 - val_loss: 0.6949 - val_accuracy: 0.6535\n",
            "Epoch 18/150\n",
            "514/514 [==============================] - 0s 167us/step - loss: 0.6425 - accuracy: 0.6595 - val_loss: 0.7002 - val_accuracy: 0.6654\n",
            "Epoch 19/150\n",
            "514/514 [==============================] - 0s 201us/step - loss: 0.6470 - accuracy: 0.6809 - val_loss: 0.6821 - val_accuracy: 0.6575\n",
            "Epoch 20/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6345 - accuracy: 0.6751 - val_loss: 0.6978 - val_accuracy: 0.6378\n",
            "Epoch 21/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6311 - accuracy: 0.6946 - val_loss: 0.6876 - val_accuracy: 0.6260\n",
            "Epoch 22/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.6340 - accuracy: 0.6751 - val_loss: 0.6867 - val_accuracy: 0.6260\n",
            "Epoch 23/150\n",
            "514/514 [==============================] - 0s 200us/step - loss: 0.6239 - accuracy: 0.6868 - val_loss: 0.6761 - val_accuracy: 0.6575\n",
            "Epoch 24/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6193 - accuracy: 0.6829 - val_loss: 0.6730 - val_accuracy: 0.6496\n",
            "Epoch 25/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6261 - accuracy: 0.6693 - val_loss: 0.6839 - val_accuracy: 0.6535\n",
            "Epoch 26/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.6143 - accuracy: 0.6809 - val_loss: 0.6503 - val_accuracy: 0.6811\n",
            "Epoch 27/150\n",
            "514/514 [==============================] - 0s 199us/step - loss: 0.6225 - accuracy: 0.6946 - val_loss: 0.6616 - val_accuracy: 0.6693\n",
            "Epoch 28/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.6240 - accuracy: 0.6809 - val_loss: 0.6984 - val_accuracy: 0.6535\n",
            "Epoch 29/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6294 - accuracy: 0.6732 - val_loss: 0.6985 - val_accuracy: 0.6575\n",
            "Epoch 30/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6246 - accuracy: 0.6634 - val_loss: 0.6662 - val_accuracy: 0.6496\n",
            "Epoch 31/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.6023 - accuracy: 0.6887 - val_loss: 0.6554 - val_accuracy: 0.6732\n",
            "Epoch 32/150\n",
            "514/514 [==============================] - 0s 205us/step - loss: 0.6054 - accuracy: 0.6751 - val_loss: 0.6658 - val_accuracy: 0.6614\n",
            "Epoch 33/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.6074 - accuracy: 0.6907 - val_loss: 0.6714 - val_accuracy: 0.6811\n",
            "Epoch 34/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6049 - accuracy: 0.6887 - val_loss: 0.6688 - val_accuracy: 0.6654\n",
            "Epoch 35/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.6014 - accuracy: 0.6868 - val_loss: 0.6290 - val_accuracy: 0.7008\n",
            "Epoch 36/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6013 - accuracy: 0.6907 - val_loss: 0.6544 - val_accuracy: 0.6654\n",
            "Epoch 37/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6023 - accuracy: 0.6907 - val_loss: 0.6423 - val_accuracy: 0.6850\n",
            "Epoch 38/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.5942 - accuracy: 0.7004 - val_loss: 0.6311 - val_accuracy: 0.6890\n",
            "Epoch 39/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.5970 - accuracy: 0.7140 - val_loss: 0.6537 - val_accuracy: 0.6811\n",
            "Epoch 40/150\n",
            "514/514 [==============================] - 0s 204us/step - loss: 0.6132 - accuracy: 0.6712 - val_loss: 0.6378 - val_accuracy: 0.6850\n",
            "Epoch 41/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5972 - accuracy: 0.6946 - val_loss: 0.6363 - val_accuracy: 0.6732\n",
            "Epoch 42/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6021 - accuracy: 0.6926 - val_loss: 0.6610 - val_accuracy: 0.6654\n",
            "Epoch 43/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5985 - accuracy: 0.6907 - val_loss: 0.6483 - val_accuracy: 0.6732\n",
            "Epoch 44/150\n",
            "514/514 [==============================] - 0s 203us/step - loss: 0.5962 - accuracy: 0.6926 - val_loss: 0.6599 - val_accuracy: 0.6693\n",
            "Epoch 45/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5948 - accuracy: 0.6770 - val_loss: 0.6374 - val_accuracy: 0.6772\n",
            "Epoch 46/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.5908 - accuracy: 0.6887 - val_loss: 0.6806 - val_accuracy: 0.6575\n",
            "Epoch 47/150\n",
            "514/514 [==============================] - 0s 215us/step - loss: 0.6077 - accuracy: 0.6809 - val_loss: 0.6617 - val_accuracy: 0.6457\n",
            "Epoch 48/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5972 - accuracy: 0.6907 - val_loss: 0.6363 - val_accuracy: 0.6811\n",
            "Epoch 49/150\n",
            "514/514 [==============================] - 0s 167us/step - loss: 0.6021 - accuracy: 0.6926 - val_loss: 0.6506 - val_accuracy: 0.6850\n",
            "Epoch 50/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6009 - accuracy: 0.6809 - val_loss: 0.6594 - val_accuracy: 0.6614\n",
            "Epoch 51/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5929 - accuracy: 0.6848 - val_loss: 0.6290 - val_accuracy: 0.6890\n",
            "Epoch 52/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.5930 - accuracy: 0.6946 - val_loss: 0.6622 - val_accuracy: 0.6614\n",
            "Epoch 53/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.5944 - accuracy: 0.6907 - val_loss: 0.6524 - val_accuracy: 0.6575\n",
            "Epoch 54/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.5948 - accuracy: 0.6907 - val_loss: 0.6309 - val_accuracy: 0.6890\n",
            "Epoch 55/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.5915 - accuracy: 0.6965 - val_loss: 0.6644 - val_accuracy: 0.6654\n",
            "Epoch 56/150\n",
            "514/514 [==============================] - 0s 170us/step - loss: 0.5943 - accuracy: 0.6848 - val_loss: 0.6291 - val_accuracy: 0.6732\n",
            "Epoch 57/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5939 - accuracy: 0.6926 - val_loss: 0.6576 - val_accuracy: 0.6654\n",
            "Epoch 58/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.5858 - accuracy: 0.6926 - val_loss: 0.6319 - val_accuracy: 0.6811\n",
            "Epoch 59/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.5979 - accuracy: 0.7004 - val_loss: 0.6416 - val_accuracy: 0.6575\n",
            "Epoch 60/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.5936 - accuracy: 0.6926 - val_loss: 0.6343 - val_accuracy: 0.6772\n",
            "Epoch 61/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6193 - accuracy: 0.6712 - val_loss: 0.6319 - val_accuracy: 0.6929\n",
            "Epoch 62/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.5923 - accuracy: 0.6926 - val_loss: 0.6467 - val_accuracy: 0.6929\n",
            "Epoch 63/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5924 - accuracy: 0.7023 - val_loss: 0.6175 - val_accuracy: 0.6772\n",
            "Epoch 64/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.5810 - accuracy: 0.7004 - val_loss: 0.6391 - val_accuracy: 0.6890\n",
            "Epoch 65/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5866 - accuracy: 0.7043 - val_loss: 0.6526 - val_accuracy: 0.6772\n",
            "Epoch 66/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5797 - accuracy: 0.6984 - val_loss: 0.6135 - val_accuracy: 0.7047\n",
            "Epoch 67/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5831 - accuracy: 0.7004 - val_loss: 0.6277 - val_accuracy: 0.6929\n",
            "Epoch 68/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.5888 - accuracy: 0.6907 - val_loss: 0.6619 - val_accuracy: 0.6772\n",
            "Epoch 69/150\n",
            "514/514 [==============================] - 0s 217us/step - loss: 0.5884 - accuracy: 0.6887 - val_loss: 0.6321 - val_accuracy: 0.6850\n",
            "Epoch 70/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5869 - accuracy: 0.6984 - val_loss: 0.6868 - val_accuracy: 0.6378\n",
            "Epoch 71/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.5862 - accuracy: 0.7062 - val_loss: 0.6634 - val_accuracy: 0.6811\n",
            "Epoch 72/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.5768 - accuracy: 0.7004 - val_loss: 0.6394 - val_accuracy: 0.6732\n",
            "Epoch 73/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.5971 - accuracy: 0.6887 - val_loss: 0.6370 - val_accuracy: 0.6890\n",
            "Epoch 74/150\n",
            "514/514 [==============================] - 0s 197us/step - loss: 0.5865 - accuracy: 0.6965 - val_loss: 0.6535 - val_accuracy: 0.6732\n",
            "Epoch 75/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.5816 - accuracy: 0.7023 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
            "Epoch 76/150\n",
            "514/514 [==============================] - 0s 215us/step - loss: 0.5778 - accuracy: 0.6946 - val_loss: 0.6354 - val_accuracy: 0.6772\n",
            "Epoch 77/150\n",
            "514/514 [==============================] - 0s 198us/step - loss: 0.5775 - accuracy: 0.6965 - val_loss: 0.6253 - val_accuracy: 0.6732\n",
            "Epoch 78/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5804 - accuracy: 0.6887 - val_loss: 0.6381 - val_accuracy: 0.6850\n",
            "Epoch 79/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.5838 - accuracy: 0.6984 - val_loss: 0.6544 - val_accuracy: 0.6496\n",
            "Epoch 80/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5821 - accuracy: 0.7004 - val_loss: 0.6413 - val_accuracy: 0.6850\n",
            "Epoch 81/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.5802 - accuracy: 0.6926 - val_loss: 0.6311 - val_accuracy: 0.6890\n",
            "Epoch 82/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5771 - accuracy: 0.6887 - val_loss: 0.6191 - val_accuracy: 0.6890\n",
            "Epoch 83/150\n",
            "514/514 [==============================] - 0s 191us/step - loss: 0.5728 - accuracy: 0.7004 - val_loss: 0.6272 - val_accuracy: 0.6969\n",
            "Epoch 84/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.5779 - accuracy: 0.7043 - val_loss: 0.6370 - val_accuracy: 0.6850\n",
            "Epoch 85/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5810 - accuracy: 0.7043 - val_loss: 0.6246 - val_accuracy: 0.7008\n",
            "Epoch 86/150\n",
            "514/514 [==============================] - 0s 191us/step - loss: 0.5745 - accuracy: 0.6984 - val_loss: 0.6358 - val_accuracy: 0.6772\n",
            "Epoch 87/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5693 - accuracy: 0.7101 - val_loss: 0.6441 - val_accuracy: 0.6654\n",
            "Epoch 88/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5723 - accuracy: 0.7082 - val_loss: 0.6324 - val_accuracy: 0.6929\n",
            "Epoch 89/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.5710 - accuracy: 0.7062 - val_loss: 0.6360 - val_accuracy: 0.6850\n",
            "Epoch 90/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.5779 - accuracy: 0.7062 - val_loss: 0.6490 - val_accuracy: 0.6614\n",
            "Epoch 91/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.5825 - accuracy: 0.6984 - val_loss: 0.6486 - val_accuracy: 0.6890\n",
            "Epoch 92/150\n",
            "514/514 [==============================] - 0s 197us/step - loss: 0.5777 - accuracy: 0.7023 - val_loss: 0.6185 - val_accuracy: 0.7008\n",
            "Epoch 93/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5731 - accuracy: 0.7101 - val_loss: 0.6348 - val_accuracy: 0.6850\n",
            "Epoch 94/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.5779 - accuracy: 0.6907 - val_loss: 0.6270 - val_accuracy: 0.6969\n",
            "Epoch 95/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5734 - accuracy: 0.7062 - val_loss: 0.6427 - val_accuracy: 0.6772\n",
            "Epoch 96/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.5798 - accuracy: 0.6926 - val_loss: 0.6498 - val_accuracy: 0.6772\n",
            "Epoch 97/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.5688 - accuracy: 0.7023 - val_loss: 0.6250 - val_accuracy: 0.6929\n",
            "Epoch 98/150\n",
            "514/514 [==============================] - 0s 203us/step - loss: 0.5774 - accuracy: 0.7101 - val_loss: 0.6152 - val_accuracy: 0.6969\n",
            "Epoch 99/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5827 - accuracy: 0.6946 - val_loss: 0.6233 - val_accuracy: 0.6850\n",
            "Epoch 100/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.5736 - accuracy: 0.7004 - val_loss: 0.6233 - val_accuracy: 0.6850\n",
            "Epoch 101/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5705 - accuracy: 0.7043 - val_loss: 0.6290 - val_accuracy: 0.6850\n",
            "Epoch 102/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5778 - accuracy: 0.7160 - val_loss: 0.6211 - val_accuracy: 0.6929\n",
            "Epoch 103/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5690 - accuracy: 0.7004 - val_loss: 0.6251 - val_accuracy: 0.6850\n",
            "Epoch 104/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5755 - accuracy: 0.7062 - val_loss: 0.6204 - val_accuracy: 0.6890\n",
            "Epoch 105/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5733 - accuracy: 0.7082 - val_loss: 0.6186 - val_accuracy: 0.6850\n",
            "Epoch 106/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.5730 - accuracy: 0.6887 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
            "Epoch 107/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.5750 - accuracy: 0.6965 - val_loss: 0.6415 - val_accuracy: 0.6929\n",
            "Epoch 108/150\n",
            "514/514 [==============================] - 0s 204us/step - loss: 0.5853 - accuracy: 0.6907 - val_loss: 0.6313 - val_accuracy: 0.6890\n",
            "Epoch 109/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5720 - accuracy: 0.6984 - val_loss: 0.6194 - val_accuracy: 0.6929\n",
            "Epoch 110/150\n",
            "514/514 [==============================] - 0s 194us/step - loss: 0.5689 - accuracy: 0.7043 - val_loss: 0.6536 - val_accuracy: 0.6811\n",
            "Epoch 111/150\n",
            "514/514 [==============================] - 0s 199us/step - loss: 0.5826 - accuracy: 0.7179 - val_loss: 0.6265 - val_accuracy: 0.6969\n",
            "Epoch 112/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.5647 - accuracy: 0.7101 - val_loss: 0.6418 - val_accuracy: 0.6811\n",
            "Epoch 113/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.5736 - accuracy: 0.7023 - val_loss: 0.6435 - val_accuracy: 0.6772\n",
            "Epoch 114/150\n",
            "514/514 [==============================] - 0s 202us/step - loss: 0.5677 - accuracy: 0.7082 - val_loss: 0.6516 - val_accuracy: 0.6575\n",
            "Epoch 115/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.5761 - accuracy: 0.7023 - val_loss: 0.6199 - val_accuracy: 0.6969\n",
            "Epoch 116/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5719 - accuracy: 0.7062 - val_loss: 0.6212 - val_accuracy: 0.6890\n",
            "Epoch 117/150\n",
            "514/514 [==============================] - 0s 222us/step - loss: 0.5777 - accuracy: 0.6887 - val_loss: 0.7183 - val_accuracy: 0.6693\n",
            "Epoch 118/150\n",
            "514/514 [==============================] - 0s 204us/step - loss: 0.5775 - accuracy: 0.7004 - val_loss: 0.6187 - val_accuracy: 0.6929\n",
            "Epoch 119/150\n",
            "514/514 [==============================] - 0s 167us/step - loss: 0.5646 - accuracy: 0.7062 - val_loss: 0.6505 - val_accuracy: 0.6850\n",
            "Epoch 120/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5654 - accuracy: 0.7062 - val_loss: 0.6167 - val_accuracy: 0.6890\n",
            "Epoch 121/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.5745 - accuracy: 0.6965 - val_loss: 0.6935 - val_accuracy: 0.6693\n",
            "Epoch 122/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5750 - accuracy: 0.6984 - val_loss: 0.6422 - val_accuracy: 0.6850\n",
            "Epoch 123/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5671 - accuracy: 0.6984 - val_loss: 0.6487 - val_accuracy: 0.6614\n",
            "Epoch 124/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.5670 - accuracy: 0.7082 - val_loss: 0.6224 - val_accuracy: 0.6969\n",
            "Epoch 125/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5719 - accuracy: 0.7062 - val_loss: 0.6231 - val_accuracy: 0.6929\n",
            "Epoch 126/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5666 - accuracy: 0.7023 - val_loss: 0.6334 - val_accuracy: 0.6850\n",
            "Epoch 127/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5697 - accuracy: 0.7043 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
            "Epoch 128/150\n",
            "514/514 [==============================] - 0s 202us/step - loss: 0.5594 - accuracy: 0.7101 - val_loss: 0.6173 - val_accuracy: 0.7008\n",
            "Epoch 129/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5752 - accuracy: 0.6926 - val_loss: 0.6312 - val_accuracy: 0.7008\n",
            "Epoch 130/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5733 - accuracy: 0.7082 - val_loss: 0.6263 - val_accuracy: 0.6929\n",
            "Epoch 131/150\n",
            "514/514 [==============================] - 0s 214us/step - loss: 0.5662 - accuracy: 0.7043 - val_loss: 0.6456 - val_accuracy: 0.6732\n",
            "Epoch 132/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5716 - accuracy: 0.7043 - val_loss: 0.6108 - val_accuracy: 0.7008\n",
            "Epoch 133/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5658 - accuracy: 0.7062 - val_loss: 0.6552 - val_accuracy: 0.6850\n",
            "Epoch 134/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5664 - accuracy: 0.7121 - val_loss: 0.6267 - val_accuracy: 0.6890\n",
            "Epoch 135/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5641 - accuracy: 0.7062 - val_loss: 0.6130 - val_accuracy: 0.6929\n",
            "Epoch 136/150\n",
            "514/514 [==============================] - 0s 197us/step - loss: 0.5659 - accuracy: 0.7101 - val_loss: 0.6463 - val_accuracy: 0.6969\n",
            "Epoch 137/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5688 - accuracy: 0.7121 - val_loss: 0.6337 - val_accuracy: 0.7047\n",
            "Epoch 138/150\n",
            "514/514 [==============================] - 0s 214us/step - loss: 0.5666 - accuracy: 0.7004 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
            "Epoch 139/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.5631 - accuracy: 0.7179 - val_loss: 0.6124 - val_accuracy: 0.6929\n",
            "Epoch 140/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5602 - accuracy: 0.7160 - val_loss: 0.6255 - val_accuracy: 0.6890\n",
            "Epoch 141/150\n",
            "514/514 [==============================] - 0s 203us/step - loss: 0.5622 - accuracy: 0.7082 - val_loss: 0.6191 - val_accuracy: 0.6890\n",
            "Epoch 142/150\n",
            "514/514 [==============================] - 0s 201us/step - loss: 0.5580 - accuracy: 0.7121 - val_loss: 0.6468 - val_accuracy: 0.6772\n",
            "Epoch 143/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.5772 - accuracy: 0.7023 - val_loss: 0.6247 - val_accuracy: 0.6890\n",
            "Epoch 144/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5753 - accuracy: 0.7062 - val_loss: 0.6211 - val_accuracy: 0.6969\n",
            "Epoch 145/150\n",
            "514/514 [==============================] - 0s 190us/step - loss: 0.5714 - accuracy: 0.7004 - val_loss: 0.6069 - val_accuracy: 0.6929\n",
            "Epoch 146/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.5705 - accuracy: 0.7023 - val_loss: 0.6854 - val_accuracy: 0.6693\n",
            "Epoch 147/150\n",
            "514/514 [==============================] - 0s 200us/step - loss: 0.5654 - accuracy: 0.7082 - val_loss: 0.6337 - val_accuracy: 0.6929\n",
            "Epoch 148/150\n",
            "514/514 [==============================] - 0s 207us/step - loss: 0.5645 - accuracy: 0.7082 - val_loss: 0.6208 - val_accuracy: 0.6890\n",
            "Epoch 149/150\n",
            "514/514 [==============================] - 0s 197us/step - loss: 0.5661 - accuracy: 0.7082 - val_loss: 0.6177 - val_accuracy: 0.7087\n",
            "Epoch 150/150\n",
            "514/514 [==============================] - 0s 213us/step - loss: 0.5627 - accuracy: 0.7179 - val_loss: 0.6149 - val_accuracy: 0.6929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6aa4647c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mwlSb_E6LbS",
        "colab_type": "text"
      },
      "source": [
        "2. Use a manual verification dataset.\n",
        "    *   Keras allows you to manually specify the dataset to use for validation during training - we use the handy train test split() function from the Python scikit-learn machine learning library to separate our data into a training and test dataset - the validation dataset can be specified to the fit() function in Keras by the validation data argument - it takes a tuple of"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1vCRIC6DcT",
        "colab_type": "code",
        "outputId": "795dca08-b74c-4bc2-d6b1-451d2f64823d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MLP with manual validation set\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# split into 67% for train and 33% for test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 514 samples, validate on 254 samples\n",
            "Epoch 1/150\n",
            "514/514 [==============================] - 0s 450us/step - loss: 1.1340 - accuracy: 0.4961 - val_loss: 0.8110 - val_accuracy: 0.6378\n",
            "Epoch 2/150\n",
            "514/514 [==============================] - 0s 167us/step - loss: 0.7366 - accuracy: 0.6576 - val_loss: 0.6977 - val_accuracy: 0.6378\n",
            "Epoch 3/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.6932 - accuracy: 0.6576 - val_loss: 0.6728 - val_accuracy: 0.6378\n",
            "Epoch 4/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6750 - accuracy: 0.6576 - val_loss: 0.6671 - val_accuracy: 0.6378\n",
            "Epoch 5/150\n",
            "514/514 [==============================] - 0s 201us/step - loss: 0.6660 - accuracy: 0.6576 - val_loss: 0.6628 - val_accuracy: 0.6378\n",
            "Epoch 6/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.6632 - accuracy: 0.6556 - val_loss: 0.6587 - val_accuracy: 0.6378\n",
            "Epoch 7/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6580 - accuracy: 0.6576 - val_loss: 0.6562 - val_accuracy: 0.6378\n",
            "Epoch 8/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.6552 - accuracy: 0.6556 - val_loss: 0.6533 - val_accuracy: 0.6378\n",
            "Epoch 9/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6520 - accuracy: 0.6576 - val_loss: 0.6519 - val_accuracy: 0.6378\n",
            "Epoch 10/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6482 - accuracy: 0.6576 - val_loss: 0.6502 - val_accuracy: 0.6378\n",
            "Epoch 11/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6555 - accuracy: 0.6556 - val_loss: 0.6516 - val_accuracy: 0.6378\n",
            "Epoch 12/150\n",
            "514/514 [==============================] - 0s 201us/step - loss: 0.6519 - accuracy: 0.6576 - val_loss: 0.6550 - val_accuracy: 0.6378\n",
            "Epoch 13/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6466 - accuracy: 0.6576 - val_loss: 0.6462 - val_accuracy: 0.6378\n",
            "Epoch 14/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6439 - accuracy: 0.6556 - val_loss: 0.6431 - val_accuracy: 0.6378\n",
            "Epoch 15/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.6396 - accuracy: 0.6556 - val_loss: 0.6434 - val_accuracy: 0.6378\n",
            "Epoch 16/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6461 - accuracy: 0.6595 - val_loss: 0.6475 - val_accuracy: 0.6378\n",
            "Epoch 17/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6389 - accuracy: 0.6576 - val_loss: 0.6443 - val_accuracy: 0.6378\n",
            "Epoch 18/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.6410 - accuracy: 0.6576 - val_loss: 0.6431 - val_accuracy: 0.6378\n",
            "Epoch 19/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6419 - accuracy: 0.6556 - val_loss: 0.6509 - val_accuracy: 0.6378\n",
            "Epoch 20/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6401 - accuracy: 0.6556 - val_loss: 0.6405 - val_accuracy: 0.6378\n",
            "Epoch 21/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.6375 - accuracy: 0.6556 - val_loss: 0.6386 - val_accuracy: 0.6378\n",
            "Epoch 22/150\n",
            "514/514 [==============================] - 0s 193us/step - loss: 0.6380 - accuracy: 0.6576 - val_loss: 0.6396 - val_accuracy: 0.6378\n",
            "Epoch 23/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.6332 - accuracy: 0.6576 - val_loss: 0.6325 - val_accuracy: 0.6378\n",
            "Epoch 24/150\n",
            "514/514 [==============================] - 0s 193us/step - loss: 0.6384 - accuracy: 0.6595 - val_loss: 0.6382 - val_accuracy: 0.6378\n",
            "Epoch 25/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6407 - accuracy: 0.6556 - val_loss: 0.6359 - val_accuracy: 0.6378\n",
            "Epoch 26/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.6354 - accuracy: 0.6576 - val_loss: 0.6301 - val_accuracy: 0.6378\n",
            "Epoch 27/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.6323 - accuracy: 0.6595 - val_loss: 0.6399 - val_accuracy: 0.6378\n",
            "Epoch 28/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6367 - accuracy: 0.6595 - val_loss: 0.6351 - val_accuracy: 0.6378\n",
            "Epoch 29/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.6355 - accuracy: 0.6576 - val_loss: 0.6333 - val_accuracy: 0.6417\n",
            "Epoch 30/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6314 - accuracy: 0.6576 - val_loss: 0.6340 - val_accuracy: 0.6417\n",
            "Epoch 31/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.6324 - accuracy: 0.6595 - val_loss: 0.6351 - val_accuracy: 0.6417\n",
            "Epoch 32/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6331 - accuracy: 0.6595 - val_loss: 0.6352 - val_accuracy: 0.6417\n",
            "Epoch 33/150\n",
            "514/514 [==============================] - 0s 202us/step - loss: 0.6344 - accuracy: 0.6595 - val_loss: 0.6346 - val_accuracy: 0.6417\n",
            "Epoch 34/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6311 - accuracy: 0.6595 - val_loss: 0.6348 - val_accuracy: 0.6417\n",
            "Epoch 35/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.6307 - accuracy: 0.6576 - val_loss: 0.6358 - val_accuracy: 0.6339\n",
            "Epoch 36/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6387 - accuracy: 0.6576 - val_loss: 0.6364 - val_accuracy: 0.6378\n",
            "Epoch 37/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6324 - accuracy: 0.6595 - val_loss: 0.6354 - val_accuracy: 0.6378\n",
            "Epoch 38/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.6287 - accuracy: 0.6576 - val_loss: 0.6394 - val_accuracy: 0.6378\n",
            "Epoch 39/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6313 - accuracy: 0.6556 - val_loss: 0.6362 - val_accuracy: 0.6378\n",
            "Epoch 40/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.6326 - accuracy: 0.6576 - val_loss: 0.6350 - val_accuracy: 0.6378\n",
            "Epoch 41/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6297 - accuracy: 0.6576 - val_loss: 0.6370 - val_accuracy: 0.6378\n",
            "Epoch 42/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.6299 - accuracy: 0.6595 - val_loss: 0.6370 - val_accuracy: 0.6378\n",
            "Epoch 43/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6323 - accuracy: 0.6595 - val_loss: 0.6375 - val_accuracy: 0.6378\n",
            "Epoch 44/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.6341 - accuracy: 0.6576 - val_loss: 0.6362 - val_accuracy: 0.6378\n",
            "Epoch 45/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6345 - accuracy: 0.6595 - val_loss: 0.6373 - val_accuracy: 0.6378\n",
            "Epoch 46/150\n",
            "514/514 [==============================] - 0s 193us/step - loss: 0.6325 - accuracy: 0.6576 - val_loss: 0.6399 - val_accuracy: 0.6378\n",
            "Epoch 47/150\n",
            "514/514 [==============================] - 0s 169us/step - loss: 0.6326 - accuracy: 0.6595 - val_loss: 0.6377 - val_accuracy: 0.6378\n",
            "Epoch 48/150\n",
            "514/514 [==============================] - 0s 172us/step - loss: 0.6286 - accuracy: 0.6595 - val_loss: 0.6378 - val_accuracy: 0.6378\n",
            "Epoch 49/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.6313 - accuracy: 0.6595 - val_loss: 0.6394 - val_accuracy: 0.6378\n",
            "Epoch 50/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6434 - accuracy: 0.6595 - val_loss: 0.6404 - val_accuracy: 0.6378\n",
            "Epoch 51/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6261 - accuracy: 0.6595 - val_loss: 0.6392 - val_accuracy: 0.6378\n",
            "Epoch 52/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.6298 - accuracy: 0.6595 - val_loss: 0.6387 - val_accuracy: 0.6378\n",
            "Epoch 53/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.6291 - accuracy: 0.6576 - val_loss: 0.6389 - val_accuracy: 0.6378\n",
            "Epoch 54/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6315 - accuracy: 0.6576 - val_loss: 0.6391 - val_accuracy: 0.6378\n",
            "Epoch 55/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6247 - accuracy: 0.6595 - val_loss: 0.6385 - val_accuracy: 0.6378\n",
            "Epoch 56/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6246 - accuracy: 0.6576 - val_loss: 0.6405 - val_accuracy: 0.6378\n",
            "Epoch 57/150\n",
            "514/514 [==============================] - 0s 170us/step - loss: 0.6232 - accuracy: 0.6576 - val_loss: 0.6401 - val_accuracy: 0.6378\n",
            "Epoch 58/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6258 - accuracy: 0.6595 - val_loss: 0.6409 - val_accuracy: 0.6378\n",
            "Epoch 59/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.6296 - accuracy: 0.6595 - val_loss: 0.6388 - val_accuracy: 0.6378\n",
            "Epoch 60/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6229 - accuracy: 0.6576 - val_loss: 0.6381 - val_accuracy: 0.6378\n",
            "Epoch 61/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6257 - accuracy: 0.6576 - val_loss: 0.6407 - val_accuracy: 0.6378\n",
            "Epoch 62/150\n",
            "514/514 [==============================] - 0s 191us/step - loss: 0.6270 - accuracy: 0.6576 - val_loss: 0.6428 - val_accuracy: 0.6378\n",
            "Epoch 63/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.6302 - accuracy: 0.6576 - val_loss: 0.6396 - val_accuracy: 0.6378\n",
            "Epoch 64/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.6220 - accuracy: 0.6576 - val_loss: 0.6397 - val_accuracy: 0.6378\n",
            "Epoch 65/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6239 - accuracy: 0.6576 - val_loss: 0.6415 - val_accuracy: 0.6378\n",
            "Epoch 66/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6233 - accuracy: 0.6576 - val_loss: 0.6402 - val_accuracy: 0.6378\n",
            "Epoch 67/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.6240 - accuracy: 0.6576 - val_loss: 0.6385 - val_accuracy: 0.6378\n",
            "Epoch 68/150\n",
            "514/514 [==============================] - 0s 193us/step - loss: 0.6215 - accuracy: 0.6576 - val_loss: 0.6416 - val_accuracy: 0.6378\n",
            "Epoch 69/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6318 - accuracy: 0.6576 - val_loss: 0.6433 - val_accuracy: 0.6378\n",
            "Epoch 70/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.6224 - accuracy: 0.6576 - val_loss: 0.6371 - val_accuracy: 0.6378\n",
            "Epoch 71/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6376 - val_accuracy: 0.6378\n",
            "Epoch 72/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6194 - accuracy: 0.6595 - val_loss: 0.6430 - val_accuracy: 0.6378\n",
            "Epoch 73/150\n",
            "514/514 [==============================] - 0s 170us/step - loss: 0.6241 - accuracy: 0.6595 - val_loss: 0.6406 - val_accuracy: 0.6378\n",
            "Epoch 74/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6198 - accuracy: 0.6595 - val_loss: 0.6407 - val_accuracy: 0.6378\n",
            "Epoch 75/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6391 - val_accuracy: 0.6378\n",
            "Epoch 76/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.6250 - accuracy: 0.6595 - val_loss: 0.6413 - val_accuracy: 0.6378\n",
            "Epoch 77/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6192 - accuracy: 0.6576 - val_loss: 0.6413 - val_accuracy: 0.6378\n",
            "Epoch 78/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6221 - accuracy: 0.6595 - val_loss: 0.6409 - val_accuracy: 0.6378\n",
            "Epoch 79/150\n",
            "514/514 [==============================] - 0s 219us/step - loss: 0.6208 - accuracy: 0.6576 - val_loss: 0.6390 - val_accuracy: 0.6378\n",
            "Epoch 80/150\n",
            "514/514 [==============================] - 0s 203us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6402 - val_accuracy: 0.6378\n",
            "Epoch 81/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6396 - val_accuracy: 0.6378\n",
            "Epoch 82/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.6194 - accuracy: 0.6595 - val_loss: 0.6412 - val_accuracy: 0.6378\n",
            "Epoch 83/150\n",
            "514/514 [==============================] - 0s 198us/step - loss: 0.6188 - accuracy: 0.6595 - val_loss: 0.6388 - val_accuracy: 0.6378\n",
            "Epoch 84/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.6197 - accuracy: 0.6595 - val_loss: 0.6394 - val_accuracy: 0.6378\n",
            "Epoch 85/150\n",
            "514/514 [==============================] - 0s 206us/step - loss: 0.6197 - accuracy: 0.6595 - val_loss: 0.6426 - val_accuracy: 0.6378\n",
            "Epoch 86/150\n",
            "514/514 [==============================] - 0s 191us/step - loss: 0.6170 - accuracy: 0.6595 - val_loss: 0.6390 - val_accuracy: 0.6378\n",
            "Epoch 87/150\n",
            "514/514 [==============================] - 0s 174us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6431 - val_accuracy: 0.6378\n",
            "Epoch 88/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6208 - accuracy: 0.6595 - val_loss: 0.6437 - val_accuracy: 0.6378\n",
            "Epoch 89/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6225 - accuracy: 0.6576 - val_loss: 0.6406 - val_accuracy: 0.6378\n",
            "Epoch 90/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.6205 - accuracy: 0.6595 - val_loss: 0.6385 - val_accuracy: 0.6378\n",
            "Epoch 91/150\n",
            "514/514 [==============================] - 0s 176us/step - loss: 0.6221 - accuracy: 0.6595 - val_loss: 0.6388 - val_accuracy: 0.6378\n",
            "Epoch 92/150\n",
            "514/514 [==============================] - 0s 204us/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.6394 - val_accuracy: 0.6378\n",
            "Epoch 93/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6167 - accuracy: 0.6615 - val_loss: 0.6409 - val_accuracy: 0.6378\n",
            "Epoch 94/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6168 - accuracy: 0.6615 - val_loss: 0.6503 - val_accuracy: 0.6378\n",
            "Epoch 95/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.6193 - accuracy: 0.6595 - val_loss: 0.6429 - val_accuracy: 0.6378\n",
            "Epoch 96/150\n",
            "514/514 [==============================] - 0s 221us/step - loss: 0.6116 - accuracy: 0.6615 - val_loss: 0.6568 - val_accuracy: 0.6378\n",
            "Epoch 97/150\n",
            "514/514 [==============================] - 0s 199us/step - loss: 0.6161 - accuracy: 0.6615 - val_loss: 0.6403 - val_accuracy: 0.6378\n",
            "Epoch 98/150\n",
            "514/514 [==============================] - 0s 189us/step - loss: 0.6109 - accuracy: 0.6634 - val_loss: 0.6834 - val_accuracy: 0.6260\n",
            "Epoch 99/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6405 - accuracy: 0.6654 - val_loss: 0.6442 - val_accuracy: 0.6378\n",
            "Epoch 100/150\n",
            "514/514 [==============================] - 0s 196us/step - loss: 0.6136 - accuracy: 0.6595 - val_loss: 0.6426 - val_accuracy: 0.6378\n",
            "Epoch 101/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.6211 - accuracy: 0.6615 - val_loss: 0.6411 - val_accuracy: 0.6378\n",
            "Epoch 102/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.6103 - accuracy: 0.6576 - val_loss: 0.6409 - val_accuracy: 0.6378\n",
            "Epoch 103/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.6110 - accuracy: 0.6615 - val_loss: 0.6482 - val_accuracy: 0.6378\n",
            "Epoch 104/150\n",
            "514/514 [==============================] - 0s 202us/step - loss: 0.6084 - accuracy: 0.6595 - val_loss: 0.6554 - val_accuracy: 0.6378\n",
            "Epoch 105/150\n",
            "514/514 [==============================] - 0s 195us/step - loss: 0.6128 - accuracy: 0.6595 - val_loss: 0.6301 - val_accuracy: 0.6535\n",
            "Epoch 106/150\n",
            "514/514 [==============================] - 0s 200us/step - loss: 0.6305 - accuracy: 0.6537 - val_loss: 0.6447 - val_accuracy: 0.6378\n",
            "Epoch 107/150\n",
            "514/514 [==============================] - 0s 202us/step - loss: 0.6115 - accuracy: 0.6615 - val_loss: 0.6407 - val_accuracy: 0.6378\n",
            "Epoch 108/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.6113 - accuracy: 0.6634 - val_loss: 0.6205 - val_accuracy: 0.6575\n",
            "Epoch 109/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.6162 - accuracy: 0.6556 - val_loss: 0.6200 - val_accuracy: 0.6575\n",
            "Epoch 110/150\n",
            "514/514 [==============================] - 0s 193us/step - loss: 0.6235 - accuracy: 0.6634 - val_loss: 0.6438 - val_accuracy: 0.6417\n",
            "Epoch 111/150\n",
            "514/514 [==============================] - 0s 194us/step - loss: 0.6166 - accuracy: 0.6654 - val_loss: 0.6267 - val_accuracy: 0.6614\n",
            "Epoch 112/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.6071 - accuracy: 0.6654 - val_loss: 0.6400 - val_accuracy: 0.6378\n",
            "Epoch 113/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.6030 - accuracy: 0.6673 - val_loss: 0.6267 - val_accuracy: 0.6614\n",
            "Epoch 114/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.5963 - accuracy: 0.6654 - val_loss: 0.6282 - val_accuracy: 0.6575\n",
            "Epoch 115/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.5982 - accuracy: 0.6770 - val_loss: 0.6290 - val_accuracy: 0.6654\n",
            "Epoch 116/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.6089 - accuracy: 0.6770 - val_loss: 0.6228 - val_accuracy: 0.6614\n",
            "Epoch 117/150\n",
            "514/514 [==============================] - 0s 198us/step - loss: 0.5908 - accuracy: 0.6790 - val_loss: 0.6306 - val_accuracy: 0.6535\n",
            "Epoch 118/150\n",
            "514/514 [==============================] - 0s 206us/step - loss: 0.5912 - accuracy: 0.6790 - val_loss: 0.6531 - val_accuracy: 0.6535\n",
            "Epoch 119/150\n",
            "514/514 [==============================] - 0s 215us/step - loss: 0.5905 - accuracy: 0.6868 - val_loss: 0.6314 - val_accuracy: 0.6693\n",
            "Epoch 120/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.6156 - accuracy: 0.6732 - val_loss: 0.6303 - val_accuracy: 0.6614\n",
            "Epoch 121/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.5900 - accuracy: 0.6848 - val_loss: 0.6291 - val_accuracy: 0.6535\n",
            "Epoch 122/150\n",
            "514/514 [==============================] - 0s 188us/step - loss: 0.6015 - accuracy: 0.6848 - val_loss: 0.6273 - val_accuracy: 0.6654\n",
            "Epoch 123/150\n",
            "514/514 [==============================] - 0s 200us/step - loss: 0.5976 - accuracy: 0.6751 - val_loss: 0.6375 - val_accuracy: 0.6575\n",
            "Epoch 124/150\n",
            "514/514 [==============================] - 0s 192us/step - loss: 0.5892 - accuracy: 0.6809 - val_loss: 0.6113 - val_accuracy: 0.6693\n",
            "Epoch 125/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5887 - accuracy: 0.6887 - val_loss: 0.6254 - val_accuracy: 0.6614\n",
            "Epoch 126/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.5896 - accuracy: 0.6751 - val_loss: 0.6274 - val_accuracy: 0.6614\n",
            "Epoch 127/150\n",
            "514/514 [==============================] - 0s 206us/step - loss: 0.5977 - accuracy: 0.6790 - val_loss: 0.6212 - val_accuracy: 0.6654\n",
            "Epoch 128/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5886 - accuracy: 0.6848 - val_loss: 0.6234 - val_accuracy: 0.6614\n",
            "Epoch 129/150\n",
            "514/514 [==============================] - 0s 183us/step - loss: 0.5888 - accuracy: 0.6946 - val_loss: 0.6171 - val_accuracy: 0.6732\n",
            "Epoch 130/150\n",
            "514/514 [==============================] - 0s 175us/step - loss: 0.5855 - accuracy: 0.6887 - val_loss: 0.6169 - val_accuracy: 0.6693\n",
            "Epoch 131/150\n",
            "514/514 [==============================] - 0s 171us/step - loss: 0.5933 - accuracy: 0.6770 - val_loss: 0.6139 - val_accuracy: 0.6732\n",
            "Epoch 132/150\n",
            "514/514 [==============================] - 0s 179us/step - loss: 0.5853 - accuracy: 0.6809 - val_loss: 0.6317 - val_accuracy: 0.6693\n",
            "Epoch 133/150\n",
            "514/514 [==============================] - 0s 185us/step - loss: 0.5905 - accuracy: 0.6751 - val_loss: 0.6221 - val_accuracy: 0.6654\n",
            "Epoch 134/150\n",
            "514/514 [==============================] - 0s 182us/step - loss: 0.5892 - accuracy: 0.6829 - val_loss: 0.6239 - val_accuracy: 0.6732\n",
            "Epoch 135/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5870 - accuracy: 0.6770 - val_loss: 0.6261 - val_accuracy: 0.6654\n",
            "Epoch 136/150\n",
            "514/514 [==============================] - 0s 184us/step - loss: 0.5879 - accuracy: 0.6868 - val_loss: 0.6317 - val_accuracy: 0.6693\n",
            "Epoch 137/150\n",
            "514/514 [==============================] - 0s 199us/step - loss: 0.5947 - accuracy: 0.6809 - val_loss: 0.6177 - val_accuracy: 0.6693\n",
            "Epoch 138/150\n",
            "514/514 [==============================] - 0s 214us/step - loss: 0.5831 - accuracy: 0.6829 - val_loss: 0.6171 - val_accuracy: 0.6811\n",
            "Epoch 139/150\n",
            "514/514 [==============================] - 0s 186us/step - loss: 0.5835 - accuracy: 0.6868 - val_loss: 0.6146 - val_accuracy: 0.6693\n",
            "Epoch 140/150\n",
            "514/514 [==============================] - 0s 173us/step - loss: 0.5924 - accuracy: 0.6654 - val_loss: 0.6117 - val_accuracy: 0.6654\n",
            "Epoch 141/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5882 - accuracy: 0.6829 - val_loss: 0.6181 - val_accuracy: 0.6614\n",
            "Epoch 142/150\n",
            "514/514 [==============================] - 0s 204us/step - loss: 0.5969 - accuracy: 0.6926 - val_loss: 0.6131 - val_accuracy: 0.6850\n",
            "Epoch 143/150\n",
            "514/514 [==============================] - 0s 181us/step - loss: 0.5757 - accuracy: 0.6907 - val_loss: 0.6226 - val_accuracy: 0.6693\n",
            "Epoch 144/150\n",
            "514/514 [==============================] - 0s 187us/step - loss: 0.5754 - accuracy: 0.6887 - val_loss: 0.6145 - val_accuracy: 0.6811\n",
            "Epoch 145/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5847 - accuracy: 0.6868 - val_loss: 0.6157 - val_accuracy: 0.6850\n",
            "Epoch 146/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5824 - accuracy: 0.6907 - val_loss: 0.6218 - val_accuracy: 0.6772\n",
            "Epoch 147/150\n",
            "514/514 [==============================] - 0s 178us/step - loss: 0.5917 - accuracy: 0.6848 - val_loss: 0.6150 - val_accuracy: 0.6654\n",
            "Epoch 148/150\n",
            "514/514 [==============================] - 0s 191us/step - loss: 0.5805 - accuracy: 0.6887 - val_loss: 0.6202 - val_accuracy: 0.6654\n",
            "Epoch 149/150\n",
            "514/514 [==============================] - 0s 180us/step - loss: 0.5829 - accuracy: 0.6868 - val_loss: 0.6137 - val_accuracy: 0.6732\n",
            "Epoch 150/150\n",
            "514/514 [==============================] - 0s 177us/step - loss: 0.5847 - accuracy: 0.6946 - val_loss: 0.6167 - val_accuracy: 0.6654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6a9b8d0f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwqKtOnm6E1-",
        "colab_type": "text"
      },
      "source": [
        "*   *Manual k-Fold Cross-Validation*\n",
        "    *   gold standard for machine learning model evaluation is k-fold cross-validation - provides a robust estimate of the performance of a model on unseen data. \n",
        "    *   it does this by splitting the training dataset into k subsets and takes turns training models on all subsets except one which\n",
        "is held out, and evaluating model performance on the held out validation dataset. \n",
        "    *   the process is repeated until all subsets are given an opportunity to be the held out validation set. \n",
        "    *   the performance measure is then averaged across all models that are created.\n",
        "    *   often not used for evaluating deep learning models because of the greater computational expense- when the problem is small enough or if you have sufficient compute resources, k-fold cross-validation can give you a less biased estimate of the performance of your model.\n",
        "    *   in the example below, we use the handy StratifiedKFold class from the scikit-learn library to split up the training dataset into 10 folds - folds are stratified, meaning that the algorithm attempts to balance the number of instances of each class in each fold. (creates and evaluates 10 models using the 10 splits; verbose output for each epoch is turned off by passing verbose=0 to the fit() and evaluate(); and the performance is printed for each model)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXU7_QPr7k5W",
        "colab_type": "code",
        "outputId": "973dafda-3105-44e4-d007-1997b592d3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# define 10-fold cross validation test harness\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "cvscores = []\n",
        "for train, test in kfold.split(X, Y):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Fit the model\n",
        "    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
        "    \n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "    \n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 79.22%\n",
            "accuracy: 79.22%\n",
            "accuracy: 72.73%\n",
            "accuracy: 67.53%\n",
            "accuracy: 70.13%\n",
            "accuracy: 70.13%\n",
            "accuracy: 75.32%\n",
            "accuracy: 74.03%\n",
            "accuracy: 69.74%\n",
            "accuracy: 73.68%\n",
            "73.17% (+/- 3.76%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDkr857Fc5SN"
      },
      "source": [
        "\n",
        "> Summary\n",
        "\n",
        "In this lesson you discovered the importance of having a robust way to estimate the performance of your deep learning models on unseen data. You learned three ways that you can estimate  performance of your deep learning models in Python using the Keras library:\n",
        "1. Automatically splitting a training dataset into train and validation datasets.\n",
        "2. Manually and explicitly defining a training and validation dataset.\n",
        "3. Evaluating performance using k-fold cross-validation, the gold standard technique."
      ]
    }
  ]
}